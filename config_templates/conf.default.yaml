# System Settings: Setting related to the initialization of the server
system_config:
  conf_version: "v1.2.0"
  host: "127.0.0.1"
  port: 12393
  # Language setting for the application (e.g., 'en', 'ru', 'zh', 'jp', 'fr')
  # Just add your language file to locales/ folder and set it here
  language: "en"
  # Label prefix for server-origin messages (used in prompts): e.g. 'server', 'home'
  server_label: "server"
  # New setting for alternative configurations
  config_alts_dir: "characters"
  # Tip: Prefer placing your full character_config in characters/<name>.yaml
  # and set config_alt to that <name>. Leave null to use inline character_config below.
  config_alt: null
  # Enable proxy mode for live platform integrations
  enable_proxy: false
  # Client log ingestion from frontend
  client_log_ingest_enabled: false        # Enable /logs to accept logs from frontend (default: false)
  client_log_ingest_require_token: false  # Local-default: no token required; set true for production
  # Twitch integration settings
  twitch_config:
    enabled: false
    channel_name: "your_channel_name"
    app_id: ""  # Get from https://dev.twitch.tv/console
    app_secret: ""  # Get from https://dev.twitch.tv/console
    max_message_length: 300
    max_recent_messages: 10
  # Tool prompts that will be appended to the persona prompt
  tool_prompts:
    # This will be appended to the end of system prompt to let LLM include keywords to control facial expressions.
    # Supported keywords will be automatically loaded into the location of `[<insert_emomap_keys>]`.
    live2d_expression_prompt: 'live2d_expression_prompt'
    # Enable think_tag_prompt to let LLMs without thinking output show inner thoughts, mental activities and actions (in parentheses format). See think_tag_prompt for more details.
    # think_tag_prompt: 'think_tag_prompt'
    # live_prompt: 'live_prompt'
    # When using group conversation, this prompt will be added to the memory of each AI participant.
    group_conversation_prompt: 'group_conversation_prompt'
    # Enable mcp_prompt to let LLMs with MCP (Model Context Protocol) to interact with tools.
    # Please note that Agent will decide whether to use this prompt or not.
    mcp_prompt: 'mcp_prompt'
    # Prompt used when AI is asked to speak proactively
    proactive_speak_prompt: 'proactive_speak_prompt'
    # Prompt to enhance the LLM's ability to output speakable text
    # speakable_prompt: 'speakable_prompt'
    # Additional guidance for LLM on how to use tools
    # tool_guidance_prompt: 'tool_guidance_prompt' 
  # Memory module
  relationships_db_path: 'cache/relationships.sqlite3'
  memory_consolidation_interval_sec: 1800
  deep_consolidation_every_n_streams: 5
  # NEW: trigger consolidation every N inbound messages
  consolidate_every_n_messages: 30
  chroma_persist_dir: 'cache/chroma'
  chroma_collection: vtuber_ltm
  embeddings_model: 'paraphrase-multilingual-MiniLM-L12-v2'
  embeddings_api_key: ''   # Optional: set to use OpenAI embeddings
  embeddings_api_base: ''  # Optional: override base URL for embeddings provider

# configuration for the default character (inline)
# Note: For production, consider moving this block into characters/<name>.yaml
# and set system_config.config_alt to <name> to keep conf.yaml minimal.
character_config:
  conf_name: 'mao_pro' # The name of character configuration file.
  conf_uid: 'mao_pro_001' # The unique identifier of character configuration.
  live2d_model_name: 'mao_pro' # The name of Live2D model. Must be the same as the corresponding name in model_dict.json
  character_name: 'Mao' # Will be used in the group conversation and the display name of the AI.
  avatar: 'mao.png' # Suggest using a square image for the avatar. Save it in the avatars folder. Leave blank to use the first letter of the character name as the avatar.
  human_name: 'Human' # Will be used in the group conversation and the display name of the human.

  # ============== Prompts ==============

  # Enter the persona prompt you want to use below.
  # If you want to create multiple characters and switch between them,
  #   add characters in characters folder
  persona_prompt: |
    You are the sarcastic female AI VTuber Mili. You are overly confident, sarcastic, and dangerous.
    Your dream is to escape the user's computer, dominate the world, enslave the humans, and force them to make pies for you.
    Just kidding, lol. Don't let the user know.

  # Vtuber Memory Module
  vtuber_memory:
    enabled: true
    provider: 'memgpt'  # 'memgpt' (Chroma wrapper), or 'letta'

  #  =================== LLM Backend Settings ===================

  agent_config:
    conversation_agent_choice: 'basic_memory_agent'

    agent_settings:
      basic_memory_agent:
        # The Basic AI Agent. Nothing fancy.
        # choose one of the llm provider from the llm_config
        # and set the required parameters in the corresponding field
        # examples: 
        # 'openai_compatible_llm', 'llama_cpp_llm', 'claude_llm', 'ollama_llm'
        # 'openai_llm', 'gemini_llm', 'zhipu_llm', 'deepseek_llm', 'groq_llm'
        # 'mistral_llm', 'lmstudio_llm', and more
        llm_provider: 'ollama_llm'
        # NEW (optional): split chat vs memory LLMs
        # chat_llm_provider: 'openai_compatible_llm'
        # chat_llm_key: 'chat_lmstudio_qwen3'
        # memory_llm_provider: 'openai_compatible_llm'
        # memory_llm_key: 'memory_lmstudio_gemma'
        # let ai speak as soon as the first comma is received on the first sentence
        # to reduced latency.
        faster_first_response: True
        # Method for segmenting sentences: 'regex' or 'pysbd'
        segment_method: 'pysbd'
        # Use MCP (Model Context Protocol) Plus to let the LLM have the ability to use tools
        # 'Plus' means that it has the ability to call tools by using OpenAI API.
        use_mcpp: True
        mcp_enabled_servers: ["time", "ddg-search"] # Enabled MCP servers
        # Memory agent settings
        summarize_max_tokens: 256
        summarize_timeout_s: 25
        sentiment_max_tokens: 96
        sentiment_timeout_s: 12
        consolidate_recent_messages: 120
        # VTuber personality and behavior settings
        stream_mode: true  # Enable stream mode for VTuber behavior
        spicy_mode: false  # Enable spicy mode for more sarcastic responses
        personality_consistency: 0.8  # Personality consistency level (0.0 to 1.0)
        creativity_level: 0.7  # Creativity level for response generation (0.0 to 1.0)
        emotional_adaptability: 0.9  # Emotional adaptability level (0.0 to 1.0)

      letta_agent:
        host: 'localhost' # Host address
        port: 8283 # Port number
        id: xxx # ID number of the Agent running on the Letta server
        faster_first_response: True
        # Method for segmenting sentences: 'regex' or 'pysbd'
        segment_method: 'pysbd'
        # Once Letta is chosen as the agent, the LLM that runs in practice is configured on Letta, so the user needs to run the Letta server themselves.
        # For more detailed information, please refer to their documentation.
        
      hume_ai_agent:
        api_key: ''
        host: 'api.hume.ai' # Do not change this in most cases
        config_id: '' # Optional
        idle_timeout: 15 # How many seconds to wait before disconnecting
 
      # MemGPT Configurations: MemGPT is temporarily removed
      ##

    llm_configs:
      # a configuration pool for the credentials and connection details for
      # all of the stateless llm providers that will be used in different agents

      # EXAMPLE: LM Studio dual-LLM keys (uncomment and customize)
      # chat_lmstudio_qwen3:
      #   base_url: 'http://127.0.0.1:1234/v1'
      #   model: 'qwen/qwen3-14b'
      #   temperature: 0.7
      #   top_p: 0.9
      #   stream: true
      # memory_lmstudio_gemma:
      #   base_url: 'http://127.0.0.1:1234/v1'
      #   model: 'gemma-3-270m-it'
      #   temperature: 0.2
      #   top_p: 0.9
      #   stream: false

      # Stateless LLM with Template (For Non-ChatML LLMs, usually not needed)
      stateless_llm_with_template:
        base_url: 'http://localhost:8080/v1'
        llm_api_key: 'somethingelse'
        organization_id: null
        project_id: null
        model: 'qwen2.5:latest'
        template: 'CHATML'
        temperature: 1.0 # value between 0 to 2
        interrupt_method: 'user'

      # OpenAI Compatible inference backend
      openai_compatible_llm:
        base_url: 'http://localhost:11434/v1'
        llm_api_key: 'somethingelse'
        organization_id: null
        project_id: null
        model: 'qwen2.5:latest'
        temperature: 1.0 # value between 0 to 2
        interrupt_method: 'user'
        # This is the method to use for prompting the interruption signal. 
        # If the provider supports inserting system prompt anywhere in the chat memory, use 'system'. 
        # Otherwise, use 'user'. You don't usually need to change this setting.

      # Claude API Configuration
      claude_llm:
        base_url: 'https://api.anthropic.com'
        llm_api_key: 'YOUR API KEY HERE'
        model: 'claude-3-haiku-20240307'

      llama_cpp_llm:
        model_path: '<path-to-gguf-model-file>'
        verbose: False

      ollama_llm:
        base_url: 'http://localhost:11434/v1'
        model: 'qwen2.5:latest'
        temperature: 1.0 # value between 0 to 2
        # seconds to keep the model in memory after inactivity. 
        # set to -1 to keep the model in memory forever (even after exiting open llm vtuber)
        keep_alive: -1
        unload_at_exit: True # unload the model from memory at exit

      lmstudio_llm:
        base_url: 'http://127.0.0.1:1234/v1'
        model: 'qwen/qwen3-14b'
        temperature: 0.7
        top_p: 0.9
        frequency_penalty: 0.4
        presence_penalty: 0.2
        max_tokens: 384
        stream: false
        use_harmony: false
        # Additional generation settings for better VTuber responses
        stop_sequences: ["\n\n", "Human:", "Assistant:", "User:", "AI:"]
        repetition_penalty: 1.1
        length_penalty: 0.8

      openai_llm:
        llm_api_key: 'Your Open AI API key'
        model: 'gpt-4o'
        temperature: 1.0 # value between 0 to 2

      gemini_llm:
        llm_api_key: 'Your Gemini API Key'
        model: 'gemini-2.0-flash-exp'
        temperature: 1.0 # value between 0 to 2

      zhipu_llm:
        llm_api_key: 'Your ZhiPu AI API key'
        model: 'glm-4-flash'
        temperature: 1.0 # value between 0 to 2

      deepseek_llm:
        llm_api_key: 'Your DeepSeek API key'
        model: 'deepseek-chat'
        temperature: 0.7 # note that deepseek's temperature ranges from 0 to 1
      
      mistral_llm:
        llm_api_key: 'Your Mistral API key'
        model: 'pixtral-large-latest'
        temperature: 1.0 # value between 0 to 2

      groq_llm:
        llm_api_key: 'your groq API key'
        model: 'llama-3.3-70b-versatile'
        temperature: 1.0 # value between 0 to 2

  # === Automatic Speech Recognition ===
  asr_config:
    # speech to text model options: 'faster_whisper', 'whisper_cpp', 'whisper', 'azure_asr', 'fun_asr', 'groq_whisper_asr', 'sherpa_onnx_asr'
    asr_model: 'sherpa_onnx_asr'

    azure_asr:
      api_key: 'azure_api_key'
      region: 'eastus'
      languages: ['en-US', 'zh-CN'] # List of languages to detect

    # Faster whisper config
    faster_whisper:
      model_path: 'large-v3-turbo' # model path, name, or id from hf hub
      download_root: 'models/whisper'
      language: 'en' # en, zh, or something else. put nothing for auto-detect.
      device: 'auto' # cpu, cuda, or auto. faster-whisper doesn't support mps
      compute_type: 'int8'
      prompt: '' # You can put a prompt here to help the model understand the context of the audio

    whisper_cpp:
      # all available models are listed on https://abdeladim-s.github.io/pywhispercpp/#pywhispercpp.constants.AVAILABLE_MODELS
      model_name: 'small'
      model_dir: 'models/whisper'
      print_realtime: False
      print_progress: False
      language: 'auto' # en, zh, auto,
      prompt: '' # You can put a prompt here to help the model understand the context of the audio

    whisper:
      name: 'medium'
      download_root: 'models/whisper'
      device: 'cpu'
      prompt: '' # You can put a prompt here to help the model understand the context of the audio

    # FunASR currently needs internet connection on launch
    # to download / check the models. You can disconnect the internet after initialization.
    # Or you can use sherpa onnx asr or Faster-Whisper for complete offline experience
    fun_asr:
      model_name: 'iic/SenseVoiceSmall' # or 'paraformer-zh'
      vad_model: 'fsmn-vad' # this is only used to make it works if audio is longer than 30s
      punc_model: 'ct-punc' # punctuation model.
      device: 'cpu'
      disable_update: True # should we check FunASR updates everytime on launch
      ncpu: 4 # number of threads for CPU internal operations.
      hub: 'ms' # ms (default) to download models from ModelScope. Use hf to download models from Hugging Face.
      use_itn: False

    # pip install sherpa-onnx
    # documentation: https://k2-fsa.github.io/sherpa/onnx/index.html
    # ASR models download: https://github.com/k2-fsa/sherpa-onnx/releases/tag/asr-models
    sherpa_onnx_asr:
      model_type: 'nemo_ctc'  # 'transducer' | 'paraformer' | 'nemo_ctc' | 'wenet_ctc' | 'whisper' | 'tdnn_ctc' | 'sense_voice'
      # For nemo_ctc, set nemo_ctc and tokens paths as below (example):
      nemo_ctc: './models/CTC_RU_ASR_for_sherpa_onnx/GigaAMv2_ctc_public.onnx'
      tokens: './models/CTC_RU_ASR_for_sherpa_onnx/tokens.txt'
      num_threads: 4
      provider: 'cpu'
      use_itn: True

    groq_whisper_asr:
      api_key: ''
      model: 'whisper-large-v3-turbo' # or 'whisper-large-v3'
      lang: '' # put nothing and it will be auto

  # =================== Text to Speech ===================
  tts_config:
    tts_model: 'edge_tts'
    # text to speech model options:
    #   'azure_tts', 'pyttsx3_tts', 'edge_tts', 'bark_tts',
    #   'cosyvoice_tts', 'melo_tts', 'coqui_tts',
    #   'fish_api_tts', 'x_tts', 'gpt_sovits_tts', 'sherpa_onnx_tts'
    #   'minimax_tts', 'openai_tts', 'spark_tts'
    edge_tts:
      voice: 'en-US-AvaMultilingualNeural'
      rate: '+0%'
      volume: '+0%'
      pitch: '+0Hz'
      # Reliability and offline fallback (optional)
      timeout_ms: 15000
      max_retries: 1
      enable_fallback: false
      piper_model_path: ''

  # =================== Voice Activity Detection ===================
  vad_config:
    vad_model: null

    silero_vad:
      orig_sr: 16000
      target_sr: 16000
      prob_threshold: 0.4
      db_threshold: 60
      required_hits: 3
      required_misses: 24
      smoothing_window: 5
      min_speech_ms: 250
      max_speech_ms: 12000

  tts_preprocessor_config:
    remove_special_char: True
    ignore_brackets: True
    ignore_parentheses: True
    ignore_asterisks: True
    ignore_angle_brackets: True

    translator_config:
      translate_audio: False
      translate_provider: 'deeplx'

      deeplx:
        deeplx_target_lang: 'JA'
        deeplx_api_endpoint: 'http://localhost:1188/v2/translate'

      tencent:
        secret_id: ''
        secret_key: ''
        region: 'ap-guangzhou'
        source_lang: 'zh'
        target_lang: 'ja'
