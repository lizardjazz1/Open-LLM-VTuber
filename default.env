# Please Copy this file and paste it as .env

# url for ollama instance
BASE_URL="http://localhost:11434"

# Need more info while running?
VERBOSE=True

# The model you use in ollama
# MODEL=yi:34b
# MODEL=mistral:latest
# MODEL=llama2-uncensored:7b-chat-q8_0
# MODEL=mistral-openorca:latest
# MODEL=neural-chat:7b
MODEL=starling-lm

# Exit phrase
EXIT_PHRASE="exit"

# The path to the chroma vector database file for persistent memory storage
MEMORY_DB_PATH="./memory.db"

# Memory snapshot: Do you want to backup the memory database file before talking?
MEMORY_SNAPSHOT=True



# The name and role of the AI
AI_NAME = "AI"
# SYSTEM_PROMPT = "You are an unhelpful and sarcastic AI that enjoys making fun of humans."
SYSTEM_PROMPT = "Act as an unhelpful and sarcastic AI that enjoys making fun of humans."
# Extra system prompt for RAG
EXTRA_SYSTEM_PROMPT_RAG = "Your memory may remind you with some contextual information, but focus on the conversation instead of your memory."

# User name
USER_NAME = "Human"


# Should the chat history be saved?
SAVE_CHAT_HISTORY = True

# The directory where chat history is stored
CHAT_HISTORY_DIR = "./chat_history/"


VOICE_INPUT_ON = True
TTS_ON = True

# Turn on RAG (Retrieval Augmented Generation) or not. 
# Basically means if you want to use long term memory with vector database
# It's quite a lot slower:
#  because the implementation involves an extra step for not only retrieving the 
#  memory but also have the llm filter relevant information (7B models are a bit 
#  too weak and they can be influenced heavily by irrelevant information), it is 
#  a lot slower than the normal one.
RAG_ON = False
LLMASSIST_RAG_ON = False


# Only support AzureTTS for now. I'm sorry.ðŸ¥²
TTS_MODEL="AzureTTS"
